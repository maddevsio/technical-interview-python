## Описание кейса
Спроектировать сервис потоковой обработки.
## Задача
1. Сервис принимает тяжелые данные (100+ MB) за раз (эндпоинт `/submit`)
2. Данные должны быть обработаны в несколько шагов (parallel & serial)
3. Необходимо оптимизировать потребление памяти / CPU
4. Обработка данных должна быть идемпотентной
5. Эндпоинт приема данных должен работать быстро (<10 seconds)
6. Метаданные должны храниться персистентно

## На что обращаем внимание
1. `/submit` должен использовать потоковую обработку для экономии памяти, хотя и больше времени занимает
2. Можно использовать celery, airflow, spark, etc. Но надо проверить знание основ потоковой обработки
3. Для идемпотентности можно использовать очереди, БД, redis/memcached, etc. Надо у любого выбранного метода выяснить минусы/плюсы
4. Как передаются данные на каждом шагу? S3, Redis, DB?
5. Знаком ли с концепцией ETL/ELT?
6. Как кандидат будет мониторить?
7. Как будет запускаться сервис? k8s, docker compose, AWS Lambda, etc.?
## Пояснения
1. 
